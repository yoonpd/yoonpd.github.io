---
title: "Conditioning and Independence"

categories:
  - Probability and Random Variables
---

# Conditioning and Independence

> í˜„ì¬ ì •ë¦¬í•˜ëŠ” ë‚´ìš©ì€ KAIST EEì˜ ì´ìœµ êµìˆ˜ë‹˜, Probability and Intorductory Random Process ê°•ì˜ë¥¼ ì°¸ê³ í•˜ì—¬ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

## Conditional Probability

- How should I change my belief about event A, if I come to know that event B occurs?

- Notation : P( A &#124; B ) â†’ probability of A, given B occurs.

- P( - &#124; B ) : probability of something, given B occurs. 
  â†’ ì´ê²ƒë„ <span style="color:red">**í•˜ë‚˜ì˜ probability law**</span>ì´ê¸° ë•Œë¬¸ì—, <span style="color:blue">**3 axioms**</span>ë¥¼ ë§Œì¡±ì‹œì¼œì•¼ í•¨

- P(A &#124;Â B)ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•´ì•¼ í• ê¹Œ?

  1. P( - &#124; B ) = P(- âˆ© B) ì´ê±¸ë¡œ í•´ë³´ì!

     - probability of A given BëŠ” both A and B occurì´ë‹ˆê¹Œ.

     - í•˜ì§€ë§Œ ì´ê²ƒì€ í‹€ë¦° ê°€ì •! â†’ **3 axiomsë¥¼ ëª¨ë‘ ë§Œì¡±í•˜ì§€ ì•ŠìŒ**

       â†’ P( Î© &#124; B ) = P(Î© âˆ© B) = P(B) != 1

  2. Normalizationì„ ì ìš©í•´ì„œ **P( - &#124; B ) = P(- âˆ© B) / P(B)**, P(B) > 0 ìœ¼ë¡œ í•´ë³´ì.

     - **3 axiomsë¥¼ ë§Œì¡±í•˜ëŠ”ê°€?**
       1. Nonnegativity : good
       2. Finite additivity and countable additivity, For any two disjoint A and C
          - P( A U C &#124; B ) = P[( A U C ) âˆ© B] / P(B) = P[(A âˆ© B) U (C âˆ© B)] / P(B)
            = P( A &#124; B ) + P( C &#124; B) 
            â†’ **P(A âˆ© B)ì™€ P(C âˆ© B) ë˜í•œ disjoint**
            â†’ FiniteëŠ” countableë¡œ í™•ì¥í•  ìˆ˜ ìˆìœ¼ë‹ˆ ìƒëµ
       3. Normalization : P( Î© &#124; B ) = **P(Î© âˆ© B) / P(B)** = P(B) / P(B) = 1
          <br/>

## Bayes' Rule and Bayesian Inference

- Porbability of A given that B occurs <span style="color:red">**VS**</span> Probability of B given that A occurs

- **Bayes' Ruleì€ 'ì¶”ë¡ (inference)'ì„ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©**ëœë‹¤.

- ì´ê²Œ ë¬´ìŠ¨ ë§ì¼ê¹Œ?

  - ì•„ë˜ì™€ ê°™ì´ ì´ë²¤íŠ¸ê°€ ìˆë‹¤ê³  ê°€ì •ì„ í•´ë³´ì.

  - A1 : HappyğŸ˜ƒ , A2 : SadğŸ˜¢ â†’ Ai : state / cause / original value
  - B : Shout! 	                        â†’ B : result / resulting action / noisy measurement
  - ê·¸ë¦¬ê³  ë‚˜ëŠ” ê´€ì°°ì´ë‚˜ ì´ì „ì˜ ë°ì´í„°ë¥¼ í†µí•´ P(A1), P(A2), P(B&#124;A1), P(B&#124;A2)ë¥¼ ì•Œê³  ìˆë‹¤.
  - ê·¸ë ‡ë‹¤ë©´ ì´ ë•Œ, P(A1 &#124;Â B)ì™€ P(A2 &#124;Â B)ë¥¼ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œ??
    â†’ <span style="color:green">**ê²°ê³¼ë§Œì„ ë´¤ì„ ë•Œ, ë¬´ì—‡ì´ ì´ ê²°ê³¼ë¥¼ ë§Œë“¤ì—ˆë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆì„ê¹Œ???**</span>
    â†’ What is the reason? == Inference!
    <br>

- ì´ëŸ¬í•œ ì¼ì„ ì–´ë–»ê²Œ í•˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° ì „ì—, ëª‡ê°œì˜ ìˆ˜í•™ì  ë£°ì„ í™•ì¸í•´ë³´ì

  1. Multiplication Rule (MR) : ì¼ë ¨ì˜ event ë°œìƒì„ ê³„ì‚°í•˜ê¸° ìœ„í•œ Rule
     - P(B &#124;Â A) = P(A âˆ© B) / P(A)
       â†’ P(A âˆ© B) = P(A)P(B &#124; A)
       â†’ P(A^c âˆ© B âˆ© C^c) = P(A^c âˆ© B) * P(C^c &#124; A^c âˆ© B)    (* : dot product) 
             = P(A^c) * P(B &#124; A^c) * P(C^c &#124; A^c âˆ© B)
     - ì´ê±¸ ì‹œê°í™”í•´ì„œ ì´í•´ë¥¼ í•´ë³´ì
       <img width="334" alt="ìŠ¤í¬ë¦°ìƒ· 2021-09-08 ì˜¤í›„ 2 01 39" src="https://user-images.githubusercontent.com/37065429/132474846-f53605c4-3d48-4881-8e1f-6a84fe643782.png">
       â†’ ì²« ë‘ ê°„ì„ ì€ Aê°€ ë°œìƒí•  í™•ë¥ , Aê°€ ë°œìƒí•˜ì§€ ì•Šì„ í™•ë¥ ì„ ì˜ë¯¸í•˜ê³ ,
       â†’ ë‹¤ìŒ ê°„ì„ ì€, Aì˜ ë°œìƒ ì—¬ë¶€ ì´í›„ Bê°€ ë°œìƒí–ˆëŠ”ì§€ ì•Ší–ˆëŠ”ì§€ë¥¼ í™•ì¸í•œë‹¤.....
       â†’ Multiplication Ruleì€ ì¼ë ¨ì˜ eventì™€ ê´€ê³„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ Aê°€ ë°œìƒí•˜ê³  Bê°€ 
           ë°œìƒí•  í™•ë¥ ì€ P(A)*P(B &#124;Â A)ì´ê³  ì´ëŠ” P(A âˆ© B)ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.
     - ì¼ë°˜í™”ë¥¼ ì‹œì¼œë³´ì
       - P(A1 âˆ© A2 âˆ© ... âˆ© An) = P(A1) * P(A2&#124;A1) * P(A3&#124;A1, A2) ... *(An&#124;A1, ..., An)
         <br>
     
  2. Total Probability Theorem (TPT)
     - **Partition ê°œë…ì„ ì‚¬ìš©í•˜ê³ , ì´ëŠ” sample space(Î©)ë¥¼ ì—¬ëŸ¬ê°œë¡œ ë‚˜ëˆˆ ê²ƒì´ë‹¤.**
       â†’ ë‚˜ëˆ ì§„ partitionì€ **mutually exclusive**í•´ì•¼ í•˜ê³ ,
           **Î© = A1 U A2 U A3 ...** ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤.
           <img width="340" alt="ìŠ¤í¬ë¦°ìƒ· 2021-09-08 ì˜¤í›„ 5 32 33" src="https://user-images.githubusercontent.com/37065429/132475126-c45d5610-34b6-4a20-bb45-4ca0f0e0fa1e.png">
     - partitionì„ ì‚¬ìš©í•  ë•Œ, P(Ai) ê·¸ë¦¬ê³  P(B &#124; Ai)ë¥¼ ì•ˆë‹¤ë©´ P(B)ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤.
       â†’ P(B) = Î£P(Ai)P(B &#124; Ai)
       â†’ ì–´ë””ì„œ ë³¸ ë“¯í•œ í˜•íƒœì§€ ì•Šì€ê°€? â†’ **[ê²°ê³¼]ì— [ê°€ì¤‘ì¹˜]ë¥¼ ê³±í•œ í˜•íƒœì´ë‹¤.**
       â†’ <span style="color:green">**ê²°ê³¼ë§Œ ë´¤ì„ ë•Œ, ì´ìœ ë¥¼ ì•Œ ìˆ˜ ìˆì„ ê²ƒë§Œ ê°™ë‹¤!!**</span>
       <br>

- ì, ì´ì œ ì´ê²ƒì„ ì ìš©í•´ì„œ Bayes' Ruleì„ ì„¤ëª…í•´ë³´ì.

  - ì•ì„œ ë§í•œ ê²ƒ ì²˜ëŸ¼ Bayes' Ruleì€ ì¶”ë¡ ì„ ë§Œë“œëŠ” ë°©ë²•ì´ê³ , ì¶”ë¡ ì€ ê²°ê³¼ë¥¼ ë³´ê³  ë¬´ì—‡ì´ ì´ ê²°ê³¼ë¥¼ ë§Œë“¤ì—ˆëŠ”ì§€ë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ì´ë‹¤.

  - ìš°ë¦¬ëŠ” sample spaceë¥¼ partitioní•œ A1, A2, A3ë¥¼ ì•Œê³  ìˆê³ , P(Ai)ì™€ P(B &#124; Ai)ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ì•Œê³  ìˆë‹¤.

  - ì¶”ë¡ ì€ ì—¬ê¸°ì„œ P(Ai &#124; B)ë¥¼ ì•Œê³  ì‹¶ì€ ê²ƒì´ë¯€ë¡œ,

    1. MRì„ ì‚¬ìš©í•´ì„œ,
       â†’ P(Ai &#124; B) = P(Ai âˆ© B) / P(B)ì„ì„ ì•Œ ìˆ˜ ìˆê³ 
    2. MRê³¼ TPTì„ ì‚¬ìš©í•´ì„œ,
       â†’ P(Ai &#124; B) = { P(Ai)P(B &#124; Ai) } / { <span style="color:red">**Î£j P(Aj)P(B &#124; Aj)**</span> } ì„ì„ ì•Œ ìˆ˜ ìˆê²Œ ëœë‹¤!!

    ![ìŠ¤í¬ë¦°ìƒ· 2021-09-08 ì˜¤í›„ 3 33 15](https://user-images.githubusercontent.com/37065429/132474710-bd437876-9fe3-4be1-b5d4-360dbccc8935.png)
    <br>

- ì‘ìš©í•´ì„œ, ì•ì„  ì˜ˆì œë¥¼ í’€ì–´ë³´ì!

  > A1 : HappyğŸ˜ƒ , A2 : SadğŸ˜¢ â†’ Ai : state / cause / original value
  >
  > B : Shout! 	                        â†’ B : result / resulting action
  > Assume :
  >   P(A1) = 0.7      ,   P(A2) = 0.3,
  >   P(B &#124; A1) = 0.3 ,   P(B &#124; A2) = 0.5

  - Calculate P(A1 &#124; B) and P(A2 &#124; B)

    - P(A1)P(B &#124; A1) = 0.7 * 0.3 = 0.21 = P(A1 âˆ© B)
    - P(A2)P(B &#124; A2) = 0.3 * 0.5 = 0.15 = P(A2 âˆ© B) = P(A1^c âˆ© B)
    - P(B) = 0.21 + 0.15 = 0.36 = P(A1 âˆ© B) + P(A1^c âˆ© B)

    1. P(A1 &#124; B) = P(B &#124; A1)P(A1) / P(B) = 0.21 / 0.36 = 0.583
    2. P(A2 &#124; B) + P(B &#124; A2)P(A2) / P(B) = 0.15 / 0.36 = 0.417

  - ì§œì”ğŸ•º ì‹ ê¸°í•˜ë‹¤
    <span style="color:purple">**ê·¼ë° ì™œ P(B) = P(A1 âˆ© B) + P(A1^c âˆ© B)ê°€ ì„±ë¦½í•˜ëŠ”ì§€ ëª¨ë¥´ê² ë‹¤!**</span>
    <br>

## Independence, Conditional Independence

- Can I ignore my knowledge about event B, when I consider event A?

- Independenceê°€ ì™œ í•„ìš”í• ê¹Œ?

  - ê°€ì •ì„ í•´ë³´ì.
    1. Event A : I get the grade A in the probability class. (my interest)
    2. Event B : My friend is rich
  - ëª…ë°±í•˜ê²Œ(ì•„ë‹ ìˆ˜ë„ ìˆì§€ë§Œ), Aì™€ BëŠ” ì—°ê´€ì´ ì—†ê³ , my interestë¥¼ ìœ„í•´ Bë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.
    â†’ <span style="color:red">**Independence makes our analysis and modeling much simpler!**</span>

- Definition : Occurence of **A provides no new information about B**. Thus, knowledge about **A does NOT change my belief about B**.
  â†’ P(B &#124; A) = P(B) = P(A âˆ© B) / P(A) â†’ <span style="color:red">**P(A âˆ© B) = P(A) X P(B) â†’ P ã…› B**</span>
  (typoraê°€ ì €ëŸ° ê¸°í˜¸ë¥¼ ì–´ë””ì„œ ê°€ì ¸ì˜¤ëŠ”ì§€ ëª¨ë¥´ê² ë‹¤. ëˆˆì¹˜ ëª»ì±˜ìœ¼ë©´ ëë‹¤.)

  1. ê·¸ëŸ¬ë©´, A and B disjointê°€ A ã…› Bì¼ê¹Œ??
     â†’ ì „í˜€ ê·¸ë ‡ì§€ ì•Šë‹¤. Aì™€ BëŠ” really dependentê´€ê³„ì´ë‹¤.
     <span style="color:red">**â†’ Aê°€ ë°œìƒ í–ˆì„ ë•Œ, Bê°€ ë°œìƒí•˜ì§€ ì•ŠìŒì„ ì•Œê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.**</span>
  2. ê·¸ëŸ¬ë©´, A ã…› Bì¼ ë•Œ, A ã…› B^cì¼ê¹Œ??
     â†’ ê·¸ë ‡ë‹¤. **Aì™€ Bê°€ independenceë¼ë©´ Aì™€ B^c ë˜í•œ independence**ì´ë‹¤.
     <br>
  
- Conditional Independence

  - Conditional probabilityì—ì„œ ë§í–ˆ ë“¯ì´ P(-)ë„ probability lawì´ê³ , P(-&#124;C) ë˜í•œ probability lawì´ë‹¤. ê·¸ëŸ¬ë©´ ì—¬ê¸°ì—ë„ independenceê°€ ì ìš©ë  ìˆ˜ ìˆë‹¤!
  - Event A, B, Cê°€ ìˆê³ , Aì™€ Bê°€ independenceí•˜ë‹¤ë©´,
    â†’ P(B &#124; A âˆ© C) = P(B &#124; C) ë¼ê³  í‘œí˜„í•  ìˆ˜ ìˆë‹¤.
    â†’ A ã…› B&#124;C â†’ P(A âˆ© B &#124; C) = P(A &#124; C) X P(B &#124; C) ì´ë‹¤.
  - ì¦ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.
    1. P(A âˆ© B &#124; C) = { P[B âˆ© (A âˆ© C)] } / P(C) = { P(A âˆ© C)P(B &#124; A âˆ© C) }/ P(C)
       = P(A &#124; C) X P(B &#124; C)
       <br>

- **A ã…› Bì™€ A ã…› B&#124;Cì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¼ê¹Œ?**

  - ë¨¼ì €, ì´ ì˜ˆì‹œë¥¼ ìƒê°í•´ë³´ì

    > Suppose that A and B are independent. If you heard that C occured, A and B are still independent?

    â†’ *ì¼ë‹¨ ë‚œ ê·¸ë ‡ë‹¤ê³  ìƒê°í–ˆë‹¤. ë‹¹ì—°íˆ ë‚´ê°€ í‹€ë ¸ë‹¤.*

  - ê°„ë‹¨í•œ ì˜ˆì‹œë¥¼ í†µí•œ ì„¤ëª…ì„ ë³´ìë©´,

    > Two independent coin tosses,
    >
    > â€‹	H1 : 1st toss is a head.
    >
    > â€‹	H2 : 2nd toss is a head.
    > â€‹	D : two tosses have different results.

    â†’ H1ê³¼ H2ëŠ” independentí•˜ë‹¤ëŠ” ê²ƒì€ ë‹¹ì—°í•˜ë‹¤.
    â†’ P(H1 &#124; D) = 1/2  ,  P(H2 &#124; D) = 1/2 ì¸ ê²ƒë„ êµ¬í•´ì§ˆ ìˆ˜ ìˆë‹¤.
    â†’ ë‚´ ìƒê°ì´ ë§ë‹¤ë©´! P(H1 âˆ© H2 &#124; D) = P(H1 &#124; D) X P(H2 &#124; D) = 1/4ê°€ ë˜ì–´ì•¼ í•œë‹¤.
        â†’ í•˜ì§€ë§Œ ë‘ ë™ì „ì´ Headì¸ë° ë‹¤ë¥¸ ê²°ê³¼ë¼ê³  ë³¼ ìˆ˜ ì—†ë‹¤. ì¦‰, 0ì´ ë‚˜ì™€ì•¼ í•œë‹¤.
        â†’ <span style="color:red">**ì¦‰, Cê°€ ë°œìƒí–ˆë‹¤ê³  í•´ì„œ Aì™€ Bê°€ ì—¬ì „íˆ independentí•˜ë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤.**</span>
    <br>

- **ê·¸ëŸ¬ë©´, A ã…› B|Cì¼ ë•Œ, A ã…› Bê°€ ì„±ë¦½ì„ í• ê¹Œ?**
  â†’ ê²°ê³¼ì ìœ¼ë¡œëŠ” ê·¸ë ‡ì§€ ì•Šë‹¤. ì˜ˆì‹œë¥¼ í†µí•´ ì„¤ëª…í•´ ë³´ê² ë‹¤.

  > ë‘ ê°œì˜ ë™ì „ì´ ìˆë‹¤. ê°ê°ì€ Blue(B)ì™€ Red(A)ì´ê³ , ë‘ ê°œ ì¤‘ í•˜ë‚˜ë¥¼ uniformlyí•˜ê²Œ ì„ íƒí•œë‹¤.
  >
  > ê·¸ëŸ° ë‹¤ìŒ ì„ íƒí•œ ë™ì „ì„ ë‘ ë²ˆ independentí•˜ê²Œ ë˜ì§„ë‹¤.
  > Assume )
  >
  > â€‹	P(Head of Blue) = 0.9 and P(Head of Red) = 0.1
  >
  > â€‹	Hi : i-th toss is head, B : Blue is selected(0.5).<img width="215" alt="ìŠ¤í¬ë¦°ìƒ· 2021-09-08 ì˜¤í›„ 5 11 47" src="https://user-images.githubusercontent.com/37065429/132474418-ed4c4b43-5c9e-4ef5-a5f8-33c2365ed55a.png">    		

  - ì—¬ê¸°ì„œ, H1 ã…› H2|BëŠ” ì„±ë¦½í•œë‹¤. independentí•˜ê²Œ ë˜ì§€ë‹ˆê¹Œ. ìˆ˜ì‹ìœ¼ë¡œ ë³´ì.
    â†’ P(H1 âˆ© H2 &#124; B) = 0.9 X 0.9 = P(H1 &#124; B) X P(H2 &#124; B) = 0.9 X 0.9
  - ê·¸ëŸ¼... <span style="color:red">**H1 ã…› H2ë„ ì„±ë¦½ì„ í• ê¹Œ? ê·¸ë ‡ì§€ ì•Šë‹¤.**</span>
    ë¨¼ì €, P(H1) = P(B)P(H1 &#124; B) + P(B^c)P(H1 &#124; B^c)
                      = (0.9 / 2) + (0.1 / 2) = 1/2
    ê·¸ëŸ¬ë©´, ë™ì „ì„ ë˜ì¡Œì„ ë•Œ ì•ë’¤ê°€ ë‚˜ì˜¤ëŠ” í™•ë¥ ì€ ë³€í•˜ì§€ ì•Šìœ¼ë‹ˆê¹Œ
            **P(H2) = P(H1)**ì´ ë‚˜ì™€ì•¼ í•œë‹¤.
            H1 ã…› H2ë¼ë©´, **P(H1 âˆ© H2) = P(H1) X P(H2) = 1/4**ê°€ ë‚˜ì™€ì•¼ í•œë‹¤.
    â†’ <span style="color:green">**P(H1 âˆ© H2) = P(B)P(H1 âˆ© H2 &#124; B) + P(B^c)P(H1 âˆ© H2 &#124; B^c)**</span>
         <span style="color:green">**= (0.9 * 0.9)/2 + (0.1 * 0.1)/2 != 1/4**</span> ì´ê¸° ë•Œë¬¸ì— H1 ã…› H2ëŠ” ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤.
    <br>
  
- Independence of Multiple Events

  - ì§€ê¸ˆê¹Œì§€ëŠ” ë‘ ê°œì˜ eventsê°€ independentí•œ conditionì„ í™•ì¸í•´ë´¤ë‹¤.

  - ê·¸ëŸ¼ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?
    <img width="553" alt="ìŠ¤í¬ë¦°ìƒ· 2021-09-08 ì˜¤í›„ 5 18 56" src="https://user-images.githubusercontent.com/37065429/132474579-364b903d-c781-4659-8c23-4f8d316d2525.png">

    ì´ë ‡ê²Œ í•˜ë©´ ëœë‹¤~